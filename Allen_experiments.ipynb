{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from gym.spaces import Dict as GymDict, Box\n",
    "from marllib import marl\n",
    "from marllib.envs.base_env import ENV_REGISTRY\n",
    "import time\n",
    "\n",
    "# importing CityLearn\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from citylearn.wrappers import StableBaselines3Wrapper\n",
    "from citylearn.wrappers import NormalizedObservationWrapper\n",
    "from pathlib import Path\n",
    "\n",
    "from marllib.envs.base_env import ENV_REGISTRY\n",
    "from add_citylearn_env import RLlibCityLearnGym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+-------------------------------+\n",
      "| Env_Name     | Check_Status   | Error_Log                                                                                                                                               | Config_File_Location                   | Env_File_Location             |\n",
      "+==============+================+=========================================================================================================================================================+========================================+===============================+\n",
      "| mpe          | Ready          | Null                                                                                                                                                    | envs/base_env/config/mpe.yaml          | envs/base_env/mpe.py          |\n",
      "+--------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+-------------------------------+\n",
      "| mamujoco     | Error          | No module named 'mujoco_py'. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.) | envs/base_env/config/mamujoco.yaml     | envs/base_env/mamujoco.py     |\n",
      "+--------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+-------------------------------+\n",
      "| smac         | Error          | No module named 'smac'                                                                                                                                  | envs/base_env/config/smac.yaml         | envs/base_env/smac.py         |\n",
      "+--------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+-------------------------------+\n",
      "| football     | Error          | No module named 'gfootball'                                                                                                                             | envs/base_env/config/football.yaml     | envs/base_env/football.py     |\n",
      "+--------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+-------------------------------+\n",
      "| magent       | Error          | cannot import name 'adversarial_pursuit_v3' from 'pettingzoo.magent'                                                                                    | envs/base_env/config/magent.yaml       | envs/base_env/magent.py       |\n",
      "+--------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+-------------------------------+\n",
      "| rware        | Error          | No module named 'rware'                                                                                                                                 | envs/base_env/config/rware.yaml        | envs/base_env/rware.py        |\n",
      "+--------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+-------------------------------+\n",
      "| lbf          | Error          | No module named 'lbforaging'                                                                                                                            | envs/base_env/config/lbf.yaml          | envs/base_env/lbf.py          |\n",
      "+--------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+-------------------------------+\n",
      "| pommerman    | Error          | No module named 'pommerman'                                                                                                                             | envs/base_env/config/pommerman.yaml    | envs/base_env/pommerman.py    |\n",
      "+--------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+-------------------------------+\n",
      "| hanabi       | Ready          | Null                                                                                                                                                    | envs/base_env/config/hanabi.yaml       | envs/base_env/hanabi.py       |\n",
      "+--------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+-------------------------------+\n",
      "| metadrive    | Error          | No module named 'metadrive'                                                                                                                             | envs/base_env/config/metadrive.yaml    | envs/base_env/metadrive.py    |\n",
      "+--------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+-------------------------------+\n",
      "| mate         | Error          | No module named 'mate'                                                                                                                                  | envs/base_env/config/mate.yaml         | envs/base_env/mate.py         |\n",
      "+--------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+-------------------------------+\n",
      "| gobigger     | Error          | No module named 'gobigger'                                                                                                                              | envs/base_env/config/gobigger.yaml     | envs/base_env/gobigger.py     |\n",
      "+--------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+-------------------------------+\n",
      "| CityLearnGym | Ready          | Null                                                                                                                                                    | envs/base_env/config/CityLearnGym.yaml | envs/base_env/CityLearnGym.py |\n",
      "+--------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+-------------------------------+\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/tmp/ipykernel_4733/2912968179.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m happo \u001b[39m=\u001b[39m marl\u001b[39m.\u001b[39malgos\u001b[39m.\u001b[39mhappo(hyperparam_source\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcommon\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m model \u001b[39m=\u001b[39m marl\u001b[39m.\u001b[39mbuild_model(env, happo, {\u001b[39m\"\u001b[39m\u001b[39mcore_arch\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmlp\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mencode_layer\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m128-128\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[0;32m----> 9\u001b[0m results \u001b[39m=\u001b[39m happo\u001b[39m.\u001b[39;49mfit(env, model, stop\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mtimesteps_total\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m8759\u001b[39;49m}, local_mode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, num_gpus\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m             num_workers\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, share_policy\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mall\u001b[39;49m\u001b[39m'\u001b[39;49m, checkpoint_freq\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m             checkpoint_end\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     13\u001b[0m results\n",
      "File \u001b[0;32m~/Desktop/work/MARLlib-1.0.3/marllib/marl/__init__.py:315\u001b[0m, in \u001b[0;36m_Algo.fit\u001b[0;34m(self, env, model, stop, **running_params)\u001b[0m\n\u001b[1;32m    313\u001b[0m     results \u001b[39m=\u001b[39m run_vd(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_dict, env_instance, model_class, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    314\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgo_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCC\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 315\u001b[0m     results \u001b[39m=\u001b[39m run_cc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig_dict, env_instance, model_class, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m    316\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnot supported type \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgo_type))\n",
      "File \u001b[0;32m~/Desktop/work/MARLlib-1.0.3/marllib/marl/algos/run_cc.py:33\u001b[0m, in \u001b[0;36mrun_cc\u001b[0;34m(exp_info, env, model, stop)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_cc\u001b[39m(exp_info, env, model, stop\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 33\u001b[0m     ray\u001b[39m.\u001b[39;49minit(local_mode\u001b[39m=\u001b[39;49mexp_info[\u001b[39m\"\u001b[39;49m\u001b[39mlocal_mode\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     35\u001b[0m     \u001b[39m########################\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[39m### environment info ###\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[39m########################\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     env_info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mget_env_info()\n",
      "File \u001b[0;32m~/anaconda3/envs/marllib/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/marllib/lib/python3.8/site-packages/ray/worker.py:836\u001b[0m, in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, _enable_object_reconstruction, _redis_max_memory, _plasma_directory, _node_ip_address, _driver_object_store_memory, _memory, _redis_password, _temp_dir, _metrics_export_port, _system_config, _tracing_startup_hook, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    835\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 836\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMaybe you called ray.init twice by accident? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    837\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mThis error can be suppressed by passing in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    838\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mignore_reinit_error=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or by calling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    839\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mray.shutdown()\u001b[39m\u001b[39m'\u001b[39m\u001b[39m prior to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mray.init()\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    841\u001b[0m _system_config \u001b[39m=\u001b[39m _system_config \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    842\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(_system_config, \u001b[39mdict\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'."
     ]
    }
   ],
   "source": [
    "ENV_REGISTRY[\"CityLearnGym\"] = RLlibCityLearnGym\n",
    "env = marl.make_env(environment_name=\"CityLearnGym\", map_name=\"CityLearn\")\n",
    "\n",
    "\n",
    "# write MASAC\n",
    "happo = marl.algos.happo(hyperparam_source=\"common\")\n",
    "model = marl.build_model(env, happo, {\"core_arch\": \"mlp\", \"encode_layer\": \"128-128\"})\n",
    "\n",
    "results = happo.fit(env, model, stop={'timesteps_total': 8759}, local_mode=True, num_gpus=1,\n",
    "            num_workers=2, share_policy='all', checkpoint_freq=500,\n",
    "            checkpoint_end=True)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'62b71_00000': {'episode_reward_max': -2.2218333333333335,\n",
       "  'episode_reward_min': -11.92724715169271,\n",
       "  'episode_reward_mean': -4.262000775655111,\n",
       "  'episode_len_mean': 1.0,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 10,\n",
       "  'policy_reward_min': {'policy_0': -5.851166666666667,\n",
       "   'policy_1': -6.370666666666667},\n",
       "  'policy_reward_max': {'policy_0': -0.8511666666666671,\n",
       "   'policy_1': -1.3706666666666665},\n",
       "  'policy_reward_mean': {'policy_0': -1.8238827883402515,\n",
       "   'policy_1': -2.4381179873148597},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [-2.5710654347737636,\n",
       "    -3.752517232259115,\n",
       "    -5.940405949910483,\n",
       "    -2.2218333333333335,\n",
       "    -2.2218333333333335,\n",
       "    -7.2218333333333335,\n",
       "    -2.2218333333333335,\n",
       "    -2.2218333333333335,\n",
       "    -5.02262602742513,\n",
       "    -3.10285063680013,\n",
       "    -6.051377400716146,\n",
       "    -10.174124440511068,\n",
       "    -2.7688384145100917,\n",
       "    -4.723884877522787,\n",
       "    -2.428187474568685,\n",
       "    -5.666062459309896,\n",
       "    -11.92724715169271,\n",
       "    -2.5607512563069665,\n",
       "    -4.635698994954428,\n",
       "    -3.7008280843098964,\n",
       "    -3.874325856526693,\n",
       "    -2.2218333333333335,\n",
       "    -2.2218333333333335,\n",
       "    -7.2218333333333335,\n",
       "    -8.8490588277181,\n",
       "    -2.2218333333333335,\n",
       "    -9.679519948323568,\n",
       "    -2.2218333333333335,\n",
       "    -2.2218333333333335,\n",
       "    -2.2218333333333335,\n",
       "    -2.2218333333333335,\n",
       "    -7.2218333333333335,\n",
       "    -2.2218333333333335,\n",
       "    -7.23857069905599,\n",
       "    -2.7543746083577476,\n",
       "    -5.730941304524739,\n",
       "    -4.400855550130209,\n",
       "    -4.530464467366537,\n",
       "    -4.504754170735677,\n",
       "    -2.2218333333333335,\n",
       "    -4.099298772176107,\n",
       "    -8.802431783040365,\n",
       "    -5.752810201009114,\n",
       "    -2.2218333333333335,\n",
       "    -6.029442128499349,\n",
       "    -3.845299062093099,\n",
       "    -2.2218333333333335,\n",
       "    -2.2218333333333335,\n",
       "    -7.2218333333333335,\n",
       "    -2.2218333333333335,\n",
       "    -2.2218333333333335,\n",
       "    -10.149129018147786,\n",
       "    -8.277441701253256,\n",
       "    -2.2218333333333335,\n",
       "    -4.784703358968099,\n",
       "    -4.293496999104819,\n",
       "    -2.2218333333333335,\n",
       "    -4.673047932942709,\n",
       "    -2.2218333333333335,\n",
       "    -2.276959142049154,\n",
       "    -2.4469416707356775,\n",
       "    -5.0158808797200525,\n",
       "    -11.715917500813802,\n",
       "    -7.2218333333333335,\n",
       "    -2.2218333333333335,\n",
       "    -2.4272059529622396,\n",
       "    -2.2218333333333335,\n",
       "    -6.853058156331381,\n",
       "    -2.2218333333333335,\n",
       "    -3.293983754475912,\n",
       "    -2.2218333333333335,\n",
       "    -2.2218333333333335,\n",
       "    -2.2218333333333335,\n",
       "    -3.8128588765462244,\n",
       "    -4.581064900716146,\n",
       "    -2.2218333333333335,\n",
       "    -3.9155673116048177,\n",
       "    -2.2218333333333335,\n",
       "    -2.2218333333333335,\n",
       "    -7.2218333333333335,\n",
       "    -2.5733946889241537,\n",
       "    -2.2218333333333335,\n",
       "    -2.2218333333333335,\n",
       "    -9.611294850667317,\n",
       "    -2.2218333333333335,\n",
       "    -5.298603352864584,\n",
       "    -5.310041341145833,\n",
       "    -2.2218333333333335,\n",
       "    -7.2218333333333335,\n",
       "    -5.84712782796224,\n",
       "    -6.799579152425131,\n",
       "    -3.069638356526693,\n",
       "    -2.2218333333333335,\n",
       "    -6.456344899495443,\n",
       "    -3.990038594563802,\n",
       "    -2.2218333333333335,\n",
       "    -3.7933756917317707,\n",
       "    -2.5565329640706382,\n",
       "    -2.2218333333333335,\n",
       "    -4.497568806966146],\n",
       "   'episode_lengths': [1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1],\n",
       "   'policy_policy_0_reward': [-0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -3.798338450113933,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -3.6519593607584637,\n",
       "    -0.8511666666666671,\n",
       "    -2.30622342936198,\n",
       "    -5.851166666666667,\n",
       "    -1.398171747843425,\n",
       "    -0.8511666666666671,\n",
       "    -1.0575208079020186,\n",
       "    -0.8511666666666671,\n",
       "    -5.830603159586589,\n",
       "    -1.1900845896402998,\n",
       "    -3.265032328287761,\n",
       "    -0.8511666666666671,\n",
       "    -2.5036591898600262,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -5.851166666666667,\n",
       "    -5.649138773600261,\n",
       "    -0.8511666666666671,\n",
       "    -5.851166666666667,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -5.851166666666667,\n",
       "    -0.8511666666666671,\n",
       "    -5.713579691569011,\n",
       "    -1.3837079416910811,\n",
       "    -1.9054238688151046,\n",
       "    -3.030188883463542,\n",
       "    -3.15979780069987,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.934567774454753,\n",
       "    -5.851166666666667,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -2.17592328898112,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -5.851166666666667,\n",
       "    -4.326327646891277,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -2.922830332438152,\n",
       "    -0.8511666666666671,\n",
       "    -3.3023812662760426,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -5.345250834147135,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -1.0152261149088546,\n",
       "    -0.8511666666666671,\n",
       "    -5.482391489664714,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -5.851166666666667,\n",
       "    -0.925732554117839,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -5.851166666666667,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -5.428912485758464,\n",
       "    -1.6989716898600264,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671,\n",
       "    -0.8511666666666671],\n",
       "   'policy_policy_1_reward': [-1.7198987681070963,\n",
       "    -2.901350565592448,\n",
       "    -2.1420674997965494,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -6.370666666666667,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -2.251683970133463,\n",
       "    -3.745153971354166,\n",
       "    -4.322957773844401,\n",
       "    -1.3706666666666665,\n",
       "    -3.87271821085612,\n",
       "    -1.3706666666666665,\n",
       "    -4.814895792643229,\n",
       "    -6.09664399210612,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -2.849661417643229,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -3.1999200541178388,\n",
       "    -1.3706666666666665,\n",
       "    -3.8283532816569013,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -1.524991007486979,\n",
       "    -1.3706666666666665,\n",
       "    -3.825517435709635,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -3.6535875040690104,\n",
       "    -1.3706666666666665,\n",
       "    -3.164730997721354,\n",
       "    -2.9512651163736976,\n",
       "    -4.901643534342448,\n",
       "    -1.3706666666666665,\n",
       "    -5.1782754618326825,\n",
       "    -1.669375773111979,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -6.370666666666667,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -4.29796235148112,\n",
       "    -3.9511140543619794,\n",
       "    -1.3706666666666665,\n",
       "    -3.9335366923014323,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -1.4257924753824869,\n",
       "    -1.5957750040690102,\n",
       "    -4.164714213053386,\n",
       "    -6.370666666666667,\n",
       "    -6.370666666666667,\n",
       "    -1.3706666666666665,\n",
       "    -1.4119798380533852,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -2.4428170878092446,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -2.9616922098795575,\n",
       "    -3.729898234049479,\n",
       "    -1.3706666666666665,\n",
       "    -3.064400644938151,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -1.647662134806315,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -3.760128184000651,\n",
       "    -1.3706666666666665,\n",
       "    -4.447436686197917,\n",
       "    -4.4588746744791665,\n",
       "    -1.3706666666666665,\n",
       "    -6.370666666666667,\n",
       "    -4.995961161295573,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -1.3706666666666665,\n",
       "    -5.605178232828776,\n",
       "    -3.138871927897135,\n",
       "    -1.3706666666666665,\n",
       "    -2.942209025065104,\n",
       "    -1.7053662974039712,\n",
       "    -1.3706666666666665,\n",
       "    -3.646402140299479]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.8991768834408482,\n",
       "   'mean_inference_ms': 0.9570303504859848,\n",
       "   'mean_action_processing_ms': 0.06843460116303653,\n",
       "   'mean_env_wait_ms': 0.27049726743164443,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'off_policy_estimator': {},\n",
       "  'num_healthy_workers': 2,\n",
       "  'timesteps_total': 8760,\n",
       "  'timesteps_this_iter': 0,\n",
       "  'agent_timesteps_total': 17520,\n",
       "  'timers': {'sample_time_ms': 262.577,\n",
       "   'sample_throughput': 38.084,\n",
       "   'load_time_ms': 0.208,\n",
       "   'load_throughput': 48182.7,\n",
       "   'learn_time_ms': 221.95,\n",
       "   'learn_throughput': 45.055,\n",
       "   'update_time_ms': 2.593},\n",
       "  'info': {'learner': {'policy_0': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "      'grad_gnorm': 3.7770162,\n",
       "      'cur_kl_coeff': 7.939328826636877e-265,\n",
       "      'cur_lr': 0.00049781250004375,\n",
       "      'total_loss': 1.9137762784957886,\n",
       "      'policy_loss': 3.631770610805774e-05,\n",
       "      'vf_loss': 1.9137399673461915,\n",
       "      'vf_explained_var': -1.1920928955078125e-07,\n",
       "      'kl': 2.23721144720912e-08,\n",
       "      'entropy': 1.2871858119964599,\n",
       "      'entropy_coeff': 0.01},\n",
       "     'model': {},\n",
       "     'custom_metrics': {}},\n",
       "    'policy_1': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "      'grad_gnorm': 2.2721171,\n",
       "      'cur_kl_coeff': 7.939328826636877e-265,\n",
       "      'cur_lr': 0.00049781250004375,\n",
       "      'total_loss': 1.8652529954910277,\n",
       "      'policy_loss': 4.501938754399859e-05,\n",
       "      'vf_loss': 1.8652080059051515,\n",
       "      'vf_explained_var': 0.0,\n",
       "      'kl': 1.4542092685587705e-08,\n",
       "      'entropy': 1.2955644130706787,\n",
       "      'entropy_coeff': 0.01},\n",
       "     'model': {},\n",
       "     'custom_metrics': {}}},\n",
       "   'num_steps_sampled': 8760,\n",
       "   'num_agent_steps_sampled': 17520,\n",
       "   'num_steps_trained': 8760,\n",
       "   'num_agent_steps_trained': 17520,\n",
       "   'num_steps_trained_this_iter': 0},\n",
       "  'done': True,\n",
       "  'episodes_total': 8760,\n",
       "  'training_iteration': 876,\n",
       "  'trial_id': '62b71_00000',\n",
       "  'experiment_id': 'fc4e6a7a0e094cbea2adb50c796aec2a',\n",
       "  'date': '2023-09-19_03-11-39',\n",
       "  'timestamp': 1695118299,\n",
       "  'time_this_iter_s': 0.24860715866088867,\n",
       "  'time_total_s': 216.54767060279846,\n",
       "  'pid': 4190,\n",
       "  'hostname': 'allenwu-super-ubuntu',\n",
       "  'node_ip': '192.168.50.163',\n",
       "  'config': {'num_workers': 2,\n",
       "   'num_envs_per_worker': 1,\n",
       "   'create_env_on_driver': False,\n",
       "   'rollout_fragment_length': 5,\n",
       "   'batch_mode': 'truncate_episodes',\n",
       "   'gamma': 0.99,\n",
       "   'lr': 0.0005,\n",
       "   'train_batch_size': 10,\n",
       "   'model': {'_use_default_native_models': False,\n",
       "    '_disable_preprocessor_api': False,\n",
       "    'fcnet_hiddens': [256, 256],\n",
       "    'fcnet_activation': 'tanh',\n",
       "    'conv_filters': None,\n",
       "    'conv_activation': 'relu',\n",
       "    'post_fcnet_hiddens': [],\n",
       "    'post_fcnet_activation': 'relu',\n",
       "    'free_log_std': False,\n",
       "    'no_final_linear': False,\n",
       "    'vf_share_layers': True,\n",
       "    'use_lstm': False,\n",
       "    'max_seq_len': 1,\n",
       "    'lstm_cell_size': 256,\n",
       "    'lstm_use_prev_action': False,\n",
       "    'lstm_use_prev_reward': False,\n",
       "    '_time_major': False,\n",
       "    'use_attention': False,\n",
       "    'attention_num_transformer_units': 1,\n",
       "    'attention_dim': 64,\n",
       "    'attention_num_heads': 1,\n",
       "    'attention_head_dim': 32,\n",
       "    'attention_memory_inference': 50,\n",
       "    'attention_memory_training': 50,\n",
       "    'attention_position_wise_mlp_dim': 32,\n",
       "    'attention_init_gru_gate_bias': 2.0,\n",
       "    'attention_use_n_prev_actions': 0,\n",
       "    'attention_use_n_prev_rewards': 0,\n",
       "    'framestack': True,\n",
       "    'dim': 84,\n",
       "    'grayscale': False,\n",
       "    'zero_mean': True,\n",
       "    'custom_model': 'Centralized_Critic_Model',\n",
       "    'custom_model_config': {'env': 'CityLearnGym',\n",
       "     'env_args': {'schema': '/home/allenwu/Desktop/work/MARLlib-1.0.3/data/schema.json',\n",
       "      'central_agent': False,\n",
       "      'map_name': 'CityLearn'},\n",
       "     'mask_flag': False,\n",
       "     'global_state_flag': False,\n",
       "     'opp_action_in_cc': True,\n",
       "     'force_coop': False,\n",
       "     'local_mode': True,\n",
       "     'share_policy': 'all',\n",
       "     'evaluation_interval': 50,\n",
       "     'framework': 'torch',\n",
       "     'num_workers': 2,\n",
       "     'num_gpus': 1,\n",
       "     'num_cpus_per_worker': 1,\n",
       "     'num_gpus_per_worker': 0,\n",
       "     'checkpoint_freq': 500,\n",
       "     'checkpoint_end': True,\n",
       "     'restore_path': {'model_path': '', 'params_path': ''},\n",
       "     'stop_iters': 9999999,\n",
       "     'stop_timesteps': 2000000,\n",
       "     'stop_reward': 999999,\n",
       "     'seed': 321,\n",
       "     'local_dir': '',\n",
       "     'model_arch_args': {'hidden_state_size': 256,\n",
       "      'core_arch': 'mlp',\n",
       "      'fc_layer': 2,\n",
       "      'out_dim_fc_0': 128,\n",
       "      'out_dim_fc_1': 64,\n",
       "      'encode_layer': '128-128'},\n",
       "     'algo_args': {'use_gae': True,\n",
       "      'gamma': 0.99,\n",
       "      'lambda': 1.0,\n",
       "      'batch_episode': 10,\n",
       "      'kl_coeff': 0.2,\n",
       "      'num_sgd_iter': 5,\n",
       "      'grad_clip': 10,\n",
       "      'clip_param': 0.3,\n",
       "      'critic_lr': 0.0005,\n",
       "      'vf_loss_coeff': 1.0,\n",
       "      'lr': 5e-07,\n",
       "      'gain': 0.01,\n",
       "      'entropy_coeff': 0.01,\n",
       "      'vf_clip_param': 10.0,\n",
       "      'min_lr_schedule': '1e-11',\n",
       "      'batch_mode': 'truncate_episodes'},\n",
       "     'algorithm': 'happo',\n",
       "     'actor_lr': 5e-07,\n",
       "     'critic_lr': 0.0005,\n",
       "     'gain': 0.01,\n",
       "     'space_obs': Dict(obs:Box([ 1.00000000e+00  1.00000000e+00  1.00000000e+00  5.59999990e+00\n",
       "       5.59999990e+00  5.59999990e+00  5.59999990e+00  1.00000000e+01\n",
       "       1.00000000e+01  1.00000000e+01  1.00000000e+01  0.00000000e+00\n",
       "       0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
       "       0.00000000e+00  0.00000000e+00  0.00000000e+00  7.03828707e-02\n",
       "       5.70000000e-02  0.00000000e+00  0.00000000e+00 -9.67676208e+02\n",
       "       2.09999993e-01  2.09999993e-01  2.09999993e-01  2.09999993e-01], [1.20000000e+01 7.00000000e+00 2.40000000e+01 3.22000008e+01\n",
       "      3.22000008e+01 3.22000008e+01 3.22000008e+01 1.00000000e+02\n",
       "      1.00000000e+02 1.00000000e+02 1.00000000e+02 1.01700000e+03\n",
       "      1.01700000e+03 1.01700000e+03 1.01700000e+03 9.53000000e+02\n",
       "      9.53000000e+02 9.53000000e+02 9.53000000e+02 2.81796217e-01\n",
       "      7.98748350e+00 9.76250000e+02 1.00000000e+00 9.67676208e+02\n",
       "      5.40000021e-01 5.40000021e-01 5.40000021e-01 5.40000021e-01], (28,), float64)),\n",
       "     'space_act': Box([-0.78125], [0.78125], (1,), float32),\n",
       "     'num_agents': 2,\n",
       "     'episode_limit': 1,\n",
       "     'policy_mapping_info': {'CityLearn': {'description': 'two buildings cooperate',\n",
       "       'team_prefix': ('building_1_', 'building_2_'),\n",
       "       'all_agents_one_policy': True,\n",
       "       'one_agent_one_policy': True}},\n",
       "     'agent_name_ls': ['building_1', 'building_2']},\n",
       "    'custom_action_dist': None,\n",
       "    'custom_preprocessor': None,\n",
       "    'lstm_use_prev_action_reward': -1},\n",
       "   'optimizer': {},\n",
       "   'horizon': 1,\n",
       "   'soft_horizon': False,\n",
       "   'no_done_at_end': False,\n",
       "   'env': 'CityLearnGym_CityLearn',\n",
       "   'observation_space': None,\n",
       "   'action_space': None,\n",
       "   'env_config': {},\n",
       "   'remote_worker_envs': False,\n",
       "   'remote_env_batch_wait_ms': 0,\n",
       "   'env_task_fn': None,\n",
       "   'render_env': False,\n",
       "   'record_env': False,\n",
       "   'clip_rewards': None,\n",
       "   'normalize_actions': True,\n",
       "   'clip_actions': False,\n",
       "   'preprocessor_pref': 'deepmind',\n",
       "   'log_level': 'WARN',\n",
       "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "   'ignore_worker_failures': False,\n",
       "   'log_sys_usage': True,\n",
       "   'fake_sampler': False,\n",
       "   'framework': 'torch',\n",
       "   'eager_tracing': False,\n",
       "   'explore': True,\n",
       "   'exploration_config': {'type': 'StochasticSampling'},\n",
       "   'evaluation_interval': 50,\n",
       "   'evaluation_num_episodes': 10,\n",
       "   'evaluation_parallel_to_training': False,\n",
       "   'in_evaluation': False,\n",
       "   'evaluation_config': {},\n",
       "   'evaluation_num_workers': 0,\n",
       "   'custom_eval_function': None,\n",
       "   'sample_async': False,\n",
       "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "   'observation_filter': 'NoFilter',\n",
       "   'synchronize_filters': True,\n",
       "   'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "    'inter_op_parallelism_threads': 2,\n",
       "    'gpu_options': {'allow_growth': True},\n",
       "    'log_device_placement': False,\n",
       "    'device_count': {'CPU': 1},\n",
       "    'allow_soft_placement': True},\n",
       "   'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "    'inter_op_parallelism_threads': 8},\n",
       "   'compress_observations': False,\n",
       "   'collect_metrics_timeout': 180,\n",
       "   'metrics_smoothing_episodes': 100,\n",
       "   'min_iter_time_s': 0,\n",
       "   'timesteps_per_iteration': 0,\n",
       "   'seed': 321,\n",
       "   'extra_python_environs_for_driver': {},\n",
       "   'extra_python_environs_for_worker': {},\n",
       "   'num_gpus': 1,\n",
       "   '_fake_gpus': False,\n",
       "   'num_cpus_per_worker': 1,\n",
       "   'num_gpus_per_worker': 0,\n",
       "   'custom_resources_per_worker': {},\n",
       "   'num_cpus_for_driver': 1,\n",
       "   'placement_strategy': 'PACK',\n",
       "   'input': 'sampler',\n",
       "   'input_config': {},\n",
       "   'actions_in_input_normalized': False,\n",
       "   'input_evaluation': ['is', 'wis'],\n",
       "   'postprocess_inputs': False,\n",
       "   'shuffle_buffer_size': 0,\n",
       "   'output': None,\n",
       "   'output_compress_columns': ['obs', 'new_obs'],\n",
       "   'output_max_file_size': 67108864,\n",
       "   'multiagent': {'policies': {'policy_0': PolicySpec(policy_class=<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>, observation_space=Dict(obs:Box([ 1.00000000e+00  1.00000000e+00  1.00000000e+00  5.59999990e+00\n",
       "       5.59999990e+00  5.59999990e+00  5.59999990e+00  1.00000000e+01\n",
       "       1.00000000e+01  1.00000000e+01  1.00000000e+01  0.00000000e+00\n",
       "       0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
       "       0.00000000e+00  0.00000000e+00  0.00000000e+00  7.03828707e-02\n",
       "       5.70000000e-02  0.00000000e+00  0.00000000e+00 -9.67676208e+02\n",
       "       2.09999993e-01  2.09999993e-01  2.09999993e-01  2.09999993e-01], [1.20000000e+01 7.00000000e+00 2.40000000e+01 3.22000008e+01\n",
       "      3.22000008e+01 3.22000008e+01 3.22000008e+01 1.00000000e+02\n",
       "      1.00000000e+02 1.00000000e+02 1.00000000e+02 1.01700000e+03\n",
       "      1.01700000e+03 1.01700000e+03 1.01700000e+03 9.53000000e+02\n",
       "      9.53000000e+02 9.53000000e+02 9.53000000e+02 2.81796217e-01\n",
       "      7.98748350e+00 9.76250000e+02 1.00000000e+00 9.67676208e+02\n",
       "      5.40000021e-01 5.40000021e-01 5.40000021e-01 5.40000021e-01], (28,), float64)), action_space=Box([-0.78125], [0.78125], (1,), float32), config={}),\n",
       "     'policy_1': PolicySpec(policy_class=<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>, observation_space=Dict(obs:Box([ 1.00000000e+00  1.00000000e+00  1.00000000e+00  5.59999990e+00\n",
       "       5.59999990e+00  5.59999990e+00  5.59999990e+00  1.00000000e+01\n",
       "       1.00000000e+01  1.00000000e+01  1.00000000e+01  0.00000000e+00\n",
       "       0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
       "       0.00000000e+00  0.00000000e+00  0.00000000e+00  7.03828707e-02\n",
       "       5.70000000e-02  0.00000000e+00  0.00000000e+00 -9.67676208e+02\n",
       "       2.09999993e-01  2.09999993e-01  2.09999993e-01  2.09999993e-01], [1.20000000e+01 7.00000000e+00 2.40000000e+01 3.22000008e+01\n",
       "      3.22000008e+01 3.22000008e+01 3.22000008e+01 1.00000000e+02\n",
       "      1.00000000e+02 1.00000000e+02 1.00000000e+02 1.01700000e+03\n",
       "      1.01700000e+03 1.01700000e+03 1.01700000e+03 9.53000000e+02\n",
       "      9.53000000e+02 9.53000000e+02 9.53000000e+02 2.81796217e-01\n",
       "      7.98748350e+00 9.76250000e+02 1.00000000e+00 9.67676208e+02\n",
       "      5.40000021e-01 5.40000021e-01 5.40000021e-01 5.40000021e-01], (28,), float64)), action_space=Box([-0.78125], [0.78125], (1,), float32), config={})},\n",
       "    'policy_map_capacity': 100,\n",
       "    'policy_map_cache': None,\n",
       "    'policy_mapping_fn': <function marllib.marl.algos.run_cc.run_cc.<locals>.<lambda>(agent_id)>,\n",
       "    'policies_to_train': None,\n",
       "    'observation_fn': None,\n",
       "    'replay_mode': 'independent',\n",
       "    'count_steps_by': 'env_steps'},\n",
       "   'logger_config': None,\n",
       "   '_tf_policy_handles_more_than_one_loss': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   'simple_optimizer': False,\n",
       "   'monitor': -1,\n",
       "   'use_critic': True,\n",
       "   'use_gae': True,\n",
       "   'lambda': 1.0,\n",
       "   'kl_coeff': 0.2,\n",
       "   'sgd_minibatch_size': 10,\n",
       "   'shuffle_sequences': True,\n",
       "   'num_sgd_iter': 5,\n",
       "   'lr_schedule': [(0, 0.0005), (2000000, 1e-11)],\n",
       "   'vf_loss_coeff': 1.0,\n",
       "   'entropy_coeff': 0.01,\n",
       "   'entropy_coeff_schedule': None,\n",
       "   'clip_param': 0.3,\n",
       "   'vf_clip_param': 10.0,\n",
       "   'grad_clip': 10,\n",
       "   'kl_target': 0.01,\n",
       "   'vf_share_layers': -1},\n",
       "  'time_since_restore': 216.54767060279846,\n",
       "  'timesteps_since_restore': 0,\n",
       "  'iterations_since_restore': 876,\n",
       "  'perf': {'cpu_util_percent': 6.2, 'ram_util_percent': 36.6},\n",
       "  'experiment_tag': '0'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(env[0].env.time_step)\n",
    "# print(env[0].env.done)\n",
    "\n",
    "# observations = env[0].env.reset()\n",
    "# print(observations)\n",
    "# print(model[0].get_actions(observations))\n",
    "\n",
    "# while not env[0].env.done:\n",
    "#     # actions, _ = model.predict(observations, deterministic=True)\n",
    "#     actions, _ = happo.compute_single_action()\n",
    "#     observations, _, _, _ = env[0].env.step(actions)\n",
    "\n",
    "# kpis = env.evaluate().pivot(index='cost_function', columns='name', values='value')\n",
    "# kpis = kpis.dropna(how='all')\n",
    "# display(kpis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this proves that you can't use env\n",
    "\n",
    "# env[0].env.net_electricity_consumption\n",
    "# env[0].env.net_electricity_consumption_without_storage\n",
    "\n",
    "# print(CostFunction.load_factor(env[0].env.net_electricity_consumption_without_storage))\n",
    "# print(CostFunction.load_factor(env2[0].env.net_electricity_consumption_without_storage))\n",
    "\n",
    "# print(env[0].env.net_electricity_consumption_without_storage)\n",
    "# print(env2[0].env.net_electricity_consumption_without_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kpis = env[0].env.evaluate().pivot(index='cost_function', columns='name', values='value')\n",
    "# kpis = kpis.dropna(how='all')\n",
    "# display(kpis)\n",
    "\n",
    "# happo.config_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
